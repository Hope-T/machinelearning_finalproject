{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77a8814",
   "metadata": {},
   "source": [
    "First, let us import and define everything that is needed. The functions tokenizer_porter(), tokenizer(), and preprocessor() are from the textbook, and will clean our data and tokenize our data for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa12d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b949958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d93ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1abdfa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                       text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30380e5c",
   "metadata": {},
   "source": [
    "Now, let us import our data, which are from Kaggle and Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "48a99e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Quote</td>\n",
       "      <td>Author</td>\n",
       "      <td>Main Tag</td>\n",
       "      <td>Other Tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>“Control of consciousness determines the quali...</td>\n",
       "      <td>― Mihaly Csikszentmihalyi, Flow: The Psycholog...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[' consciousness, happiness, quality-of-life']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>“Copulation is no more foul to me than death is.”</td>\n",
       "      <td>― Walt Whitman, Leaves of Grass: The First (18...</td>\n",
       "      <td>death</td>\n",
       "      <td>[' death, sex']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>“Hope is a dream of which we long to have. Don...</td>\n",
       "      <td>― Peace Gypsy, Souls Deep</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[' dreamers, dreams, happiness, love, peace']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>“True devotion and humility is when you carele...</td>\n",
       "      <td>― Michael Bassey Johnson</td>\n",
       "      <td>truth</td>\n",
       "      <td>[' acceptance, achilles-heel, admiration, affe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                                  1  \\\n",
       "0  NaN                                              Quote   \n",
       "1  0.0  “Control of consciousness determines the quali...   \n",
       "2  1.0  “Copulation is no more foul to me than death is.”   \n",
       "3  2.0  “Hope is a dream of which we long to have. Don...   \n",
       "4  3.0  “True devotion and humility is when you carele...   \n",
       "\n",
       "                                                   2          3  \\\n",
       "0                                             Author   Main Tag   \n",
       "1  ― Mihaly Csikszentmihalyi, Flow: The Psycholog...  happiness   \n",
       "2  ― Walt Whitman, Leaves of Grass: The First (18...      death   \n",
       "3                          ― Peace Gypsy, Souls Deep  happiness   \n",
       "4                           ― Michael Bassey Johnson      truth   \n",
       "\n",
       "                                                   4  \n",
       "0                                         Other Tags  \n",
       "1     [' consciousness, happiness, quality-of-life']  \n",
       "2                                    [' death, sex']  \n",
       "3      [' dreamers, dreams, happiness, love, peace']  \n",
       "4  [' acceptance, achilles-heel, admiration, affe...  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"book_quotes.csv\",header=None,encoding='utf-8')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bbdca393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_key</td>\n",
       "      <td>game/0</td>\n",
       "      <td>url</td>\n",
       "      <td>description</td>\n",
       "      <td>author</td>\n",
       "      <td>title</td>\n",
       "      <td>text</td>\n",
       "      <td>game/1</td>\n",
       "      <td>game/2</td>\n",
       "      <td>game/3</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.imperial-library.info/content/dyin...</td>\n",
       "      <td>Morrowind</td>\n",
       "      <td>https://www.imperial-library.info/content/dyin...</td>\n",
       "      <td>The  last words of a world-renowned archaeolog...</td>\n",
       "      <td>Indie</td>\n",
       "      <td>A Dying Man's Last Words</td>\n",
       "      <td>It's been many days since the collapse. I have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.imperial-library.info/content/fair...</td>\n",
       "      <td>Morrowind</td>\n",
       "      <td>https://www.imperial-library.info/content/fair...</td>\n",
       "      <td>Fair warning to those who would enter the Grea...</td>\n",
       "      <td>Cumanya</td>\n",
       "      <td>A Fair Warning</td>\n",
       "      <td>This being an account of my limited journeys i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.imperial-library.info/content/game...</td>\n",
       "      <td>Morrowind</td>\n",
       "      <td>https://www.imperial-library.info/content/game...</td>\n",
       "      <td>A published letter from an anonymous spy about...</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>A Game at Dinner</td>\n",
       "      <td>A GAME AT DINNER\\r\\nby\\r\\nAn Anonymous Spy\\r\\n...</td>\n",
       "      <td>Oblivion</td>\n",
       "      <td>Skyrim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.imperial-library.info/content/hypo...</td>\n",
       "      <td>Morrowind</td>\n",
       "      <td>https://www.imperial-library.info/content/hypo...</td>\n",
       "      <td>An amusing play about a bunch of backstabbing ...</td>\n",
       "      <td>Anthil Morvir</td>\n",
       "      <td>A Hypothetical Treachery</td>\n",
       "      <td>A Hypothetical Treachery\\r\\nA One Act Play\\r\\n...</td>\n",
       "      <td>Oblivion</td>\n",
       "      <td>Skyrim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0          1   \\\n",
       "0                                               _key     game/0   \n",
       "1  https://www.imperial-library.info/content/dyin...  Morrowind   \n",
       "2  https://www.imperial-library.info/content/fair...  Morrowind   \n",
       "3  https://www.imperial-library.info/content/game...  Morrowind   \n",
       "4  https://www.imperial-library.info/content/hypo...  Morrowind   \n",
       "\n",
       "                                                  2   \\\n",
       "0                                                url   \n",
       "1  https://www.imperial-library.info/content/dyin...   \n",
       "2  https://www.imperial-library.info/content/fair...   \n",
       "3  https://www.imperial-library.info/content/game...   \n",
       "4  https://www.imperial-library.info/content/hypo...   \n",
       "\n",
       "                                                  3              4   \\\n",
       "0                                        description         author   \n",
       "1  The  last words of a world-renowned archaeolog...          Indie   \n",
       "2  Fair warning to those who would enter the Grea...        Cumanya   \n",
       "3  A published letter from an anonymous spy about...      Anonymous   \n",
       "4  An amusing play about a bunch of backstabbing ...  Anthil Morvir   \n",
       "\n",
       "                         5   \\\n",
       "0                     title   \n",
       "1  A Dying Man's Last Words   \n",
       "2            A Fair Warning   \n",
       "3          A Game at Dinner   \n",
       "4  A Hypothetical Treachery   \n",
       "\n",
       "                                                  6         7       8   \\\n",
       "0                                               text    game/1  game/2   \n",
       "1  It's been many days since the collapse. I have...       NaN     NaN   \n",
       "2  This being an account of my limited journeys i...       NaN     NaN   \n",
       "3  A GAME AT DINNER\\r\\nby\\r\\nAn Anonymous Spy\\r\\n...  Oblivion  Skyrim   \n",
       "4  A Hypothetical Treachery\\r\\nA One Act Play\\r\\n...  Oblivion  Skyrim   \n",
       "\n",
       "       9       10  \n",
       "0  game/3  series  \n",
       "1     NaN     NaN  \n",
       "2     NaN     NaN  \n",
       "3     NaN     NaN  \n",
       "4     NaN     NaN  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"elder_scrolls_dialogue.csv\",header=None,encoding='utf-8')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0e3ffbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Catchphrase</td>\n",
       "      <td>Movie Name</td>\n",
       "      <td>Context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beetlejuice, Beetlejuice, Beetlejuice!</td>\n",
       "      <td>BEETLEJUICE</td>\n",
       "      <td>Lydia, summoning Beetlejuice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's showtime!</td>\n",
       "      <td>BEETLEJUICE</td>\n",
       "      <td>Beetlejuice, being summoned.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They're heeeere!</td>\n",
       "      <td>POLTERGEIST</td>\n",
       "      <td>Carol Anne Freeling, notifying her parents of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey you guys!</td>\n",
       "      <td>THE GOONIES</td>\n",
       "      <td>Sloth, calling the attention of the children ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0             1  \\\n",
       "0                             Catchphrase    Movie Name   \n",
       "1  Beetlejuice, Beetlejuice, Beetlejuice!   BEETLEJUICE   \n",
       "2                          It's showtime!   BEETLEJUICE   \n",
       "3                        They're heeeere!   POLTERGEIST   \n",
       "4                           Hey you guys!   THE GOONIES   \n",
       "\n",
       "                                                   2  \n",
       "0                                            Context  \n",
       "1                       Lydia, summoning Beetlejuice  \n",
       "2                       Beetlejuice, being summoned.  \n",
       "3   Carol Anne Freeling, notifying her parents of...  \n",
       "4   Sloth, calling the attention of the children ...  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"movie_catchphrases.csv\",header=None,encoding='utf-8')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e1e720b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cz/nvn3m4_17v7_bq36f06k_lk40000gn/T/ipykernel_40730/2169253767.py:1: DtypeWarning: Columns (5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,25,26,27,28,31,33,34,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df4 = pd.read_csv(\"movie_lines.csv\",header=None,encoding='utf-8')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>They do not!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>They do to!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I hope so.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>She okay?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Let's go.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2        3             4    5    6    7    8    9   ...   26  \\\n",
       "0  L1045  u0  m0   BIANCA  They do not!  NaN  NaN  NaN  NaN  NaN  ...  NaN   \n",
       "1  L1044  u2  m0  CAMERON   They do to!  NaN  NaN  NaN  NaN  NaN  ...  NaN   \n",
       "2   L985  u0  m0   BIANCA    I hope so.  NaN  NaN  NaN  NaN  NaN  ...  NaN   \n",
       "3   L984  u2  m0  CAMERON     She okay?  NaN  NaN  NaN  NaN  NaN  ...  NaN   \n",
       "4   L925  u0  m0   BIANCA     Let's go.  NaN  NaN  NaN  NaN  NaN  ...  NaN   \n",
       "\n",
       "    27   28  29  30   31  32   33   34   35  \n",
       "0  NaN  NaN NaN NaN  NaN NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN NaN NaN  NaN NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN NaN NaN  NaN NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN NaN NaN  NaN NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN NaN NaN  NaN NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv(\"movie_lines.csv\",header=None,encoding='utf-8')\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d6bd2566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td>MOVIE</td>\n",
       "      <td>YEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Frankly, my dear, I don't give a damn.</td>\n",
       "      <td>GONE WITH THE WIND</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm gonna make him an offer he can't refuse.</td>\n",
       "      <td>THE GODFATHER</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You don't understand! I coulda had class. I co...</td>\n",
       "      <td>ON THE WATERFRONT</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Toto, I've a feeling we're not in Kansas anymore.</td>\n",
       "      <td>THE WIZARD OF OZ</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1                   2  \\\n",
       "0  #                                              QUOTE               MOVIE   \n",
       "1  1             Frankly, my dear, I don't give a damn.  GONE WITH THE WIND   \n",
       "2  2       I'm gonna make him an offer he can't refuse.       THE GODFATHER   \n",
       "3  3  You don't understand! I coulda had class. I co...   ON THE WATERFRONT   \n",
       "4  4  Toto, I've a feeling we're not in Kansas anymore.    THE WIZARD OF OZ   \n",
       "\n",
       "      3  \n",
       "0  YEAR  \n",
       "1  1939  \n",
       "2  1972  \n",
       "3  1954  \n",
       "4  1939  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = pd.read_csv(\"movie_quotes.csv\",header=None,encoding='utf-8')\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6eaed29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>speaker</td>\n",
       "      <td>listener</td>\n",
       "      <td>text</td>\n",
       "      <td>animation</td>\n",
       "      <td>comment</td>\n",
       "      <td>next</td>\n",
       "      <td>previous</td>\n",
       "      <td>source_dlg</td>\n",
       "      <td>audiofile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Take care of yourself. The price of kolto tank...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11000_.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Selkath put a bunch of export restrictions...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11001_.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I hear that Manaan is no longer shipping kolto...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11002_.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If you have kolto tanks, use them sparingly. I...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11003_.mp3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                     1         2  \\\n",
       "0  id               speaker  listener   \n",
       "1   0  Anchorhead Tradesman       NaN   \n",
       "2   1  Anchorhead Tradesman       NaN   \n",
       "3   2  Anchorhead Tradesman       NaN   \n",
       "4   3  Anchorhead Tradesman       NaN   \n",
       "\n",
       "                                                   3          4        5  \\\n",
       "0                                               text  animation  comment   \n",
       "1  Take care of yourself. The price of kolto tank...         []      NaN   \n",
       "2  The Selkath put a bunch of export restrictions...         []      NaN   \n",
       "3  I hear that Manaan is no longer shipping kolto...         []      NaN   \n",
       "4  If you have kolto tanks, use them sparingly. I...         []      NaN   \n",
       "\n",
       "      6         7              8                     9  \n",
       "0  next  previous     source_dlg             audiofile  \n",
       "1    []    [None]  tat17_news_01  NM17AANEWS11000_.mp3  \n",
       "2    []    [None]  tat17_news_01  NM17AANEWS11001_.mp3  \n",
       "3    []    [None]  tat17_news_01  NM17AANEWS11002_.mp3  \n",
       "4    []    [None]  tat17_news_01  NM17AANEWS11003_.mp3  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = pd.read_csv(\"star_wars_dialogue.csv\",header=None,encoding='utf-8')\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "76c0b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>speaker</td>\n",
       "      <td>text</td>\n",
       "      <td>dialogtype</td>\n",
       "      <td>quest_displayname</td>\n",
       "      <td>quest_name</td>\n",
       "      <td>questfile</td>\n",
       "      <td>speaker_unit</td>\n",
       "      <td>unitfile</td>\n",
       "      <td>raw_text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Felicia</td>\n",
       "      <td>You seem weary - perhaps you are in need of a ...</td>\n",
       "      <td>complete</td>\n",
       "      <td>The Adventure Continues</td>\n",
       "      <td>global_newgameplus</td>\n",
       "      <td>QUESTS/GLOBAL_NEWGAMEPLUS.DAT</td>\n",
       "      <td>RETIREMENTCHARACTERQUEST</td>\n",
       "      <td>MEDIA/UNITS/MONSTERS/NPC/RETIREMENTCHARACTERQU...</td>\n",
       "      <td>You seem weary - perhaps you are in need of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bounty Board</td>\n",
       "      <td>BOUNTY!\\nThe Sturmbeorn raiders are a threat t...</td>\n",
       "      <td>intro</td>\n",
       "      <td>Bounty: Sturmbeorn!</td>\n",
       "      <td>a1z1-bountyboard2</td>\n",
       "      <td>QUESTS/A1Z1-BOUNTYBOARD2.DAT</td>\n",
       "      <td>BOUNTYBOARD</td>\n",
       "      <td>MEDIA/UNITS/MONSTERS/BOUNTYBOARD/BOUNTYBOARD.DAT</td>\n",
       "      <td>BOUNTY!\\n\\nThe Sturmbeorn raiders are a threat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bounty Board</td>\n",
       "      <td>The Sturmbeorn have been conducting raids in n...</td>\n",
       "      <td>return</td>\n",
       "      <td>Bounty: Sturmbeorn!</td>\n",
       "      <td>a1z1-bountyboard2</td>\n",
       "      <td>QUESTS/A1Z1-BOUNTYBOARD2.DAT</td>\n",
       "      <td>BOUNTYBOARD</td>\n",
       "      <td>MEDIA/UNITS/MONSTERS/BOUNTYBOARD/BOUNTYBOARD.DAT</td>\n",
       "      <td>The Sturmbeorn have been conducting raids in n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bounty Board</td>\n",
       "      <td>The Empire thanks you for your valuable servic...</td>\n",
       "      <td>complete</td>\n",
       "      <td>Bounty: Sturmbeorn!</td>\n",
       "      <td>a1z1-bountyboard2</td>\n",
       "      <td>QUESTS/A1Z1-BOUNTYBOARD2.DAT</td>\n",
       "      <td>BOUNTYBOARD</td>\n",
       "      <td>MEDIA/UNITS/MONSTERS/BOUNTYBOARD/BOUNTYBOARD.DAT</td>\n",
       "      <td>The Empire thanks you for your valuable servic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0                                                  1  \\\n",
       "0       speaker                                               text   \n",
       "1       Felicia  You seem weary - perhaps you are in need of a ...   \n",
       "2  Bounty Board  BOUNTY!\\nThe Sturmbeorn raiders are a threat t...   \n",
       "3  Bounty Board  The Sturmbeorn have been conducting raids in n...   \n",
       "4  Bounty Board  The Empire thanks you for your valuable servic...   \n",
       "\n",
       "            2                        3                   4  \\\n",
       "0  dialogtype        quest_displayname          quest_name   \n",
       "1    complete  The Adventure Continues  global_newgameplus   \n",
       "2       intro      Bounty: Sturmbeorn!   a1z1-bountyboard2   \n",
       "3      return      Bounty: Sturmbeorn!   a1z1-bountyboard2   \n",
       "4    complete      Bounty: Sturmbeorn!   a1z1-bountyboard2   \n",
       "\n",
       "                               5                         6  \\\n",
       "0                      questfile              speaker_unit   \n",
       "1  QUESTS/GLOBAL_NEWGAMEPLUS.DAT  RETIREMENTCHARACTERQUEST   \n",
       "2   QUESTS/A1Z1-BOUNTYBOARD2.DAT               BOUNTYBOARD   \n",
       "3   QUESTS/A1Z1-BOUNTYBOARD2.DAT               BOUNTYBOARD   \n",
       "4   QUESTS/A1Z1-BOUNTYBOARD2.DAT               BOUNTYBOARD   \n",
       "\n",
       "                                                   7  \\\n",
       "0                                           unitfile   \n",
       "1  MEDIA/UNITS/MONSTERS/NPC/RETIREMENTCHARACTERQU...   \n",
       "2   MEDIA/UNITS/MONSTERS/BOUNTYBOARD/BOUNTYBOARD.DAT   \n",
       "3   MEDIA/UNITS/MONSTERS/BOUNTYBOARD/BOUNTYBOARD.DAT   \n",
       "4   MEDIA/UNITS/MONSTERS/BOUNTYBOARD/BOUNTYBOARD.DAT   \n",
       "\n",
       "                                                   8  \n",
       "0                                           raw_text  \n",
       "1  You seem weary - perhaps you are in need of a ...  \n",
       "2  BOUNTY!\\n\\nThe Sturmbeorn raiders are a threat...  \n",
       "3  The Sturmbeorn have been conducting raids in n...  \n",
       "4  The Empire thanks you for your valuable servic...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7 = pd.read_csv(\"torchlight_dialogue.csv\",header=None,encoding='utf-8')\n",
    "df7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da217139",
   "metadata": {},
   "source": [
    "Let us see how much data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208dc505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_book_lines = len(df1)\n",
    "num_of_book_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8616594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293620"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_movie_lines = len(df3) + len(df4) + len(df5)\n",
    "num_of_movie_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5b6c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35670"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_videogame_lines = len(df2) + len(df6) + len(df7)\n",
    "num_of_videogame_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee12b6",
   "metadata": {},
   "source": [
    "There's a lot of data, so let's create a new dataset with 500 randomly chosen lines of dialogue from each category. Let's see how high the accuracy can be on a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5fa131f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's drop the column labels in row 0 to make sure we don't select those for training.\n",
    "df1 = df1.drop([0,1])\n",
    "df2 = df2.drop([0,1])\n",
    "df3 = df3.drop([0,1])\n",
    "df4 = df4.drop([0,1])\n",
    "df5 = df5.drop([0,1])\n",
    "df6 = df6.drop([0,1])\n",
    "df7 = df7.drop([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b0fb8",
   "metadata": {},
   "source": [
    "We only have one dataset for books, so let's randomly select 500 quotes from the dataset. We want to drop irrelevant columns, rename the last column, and add a new column for the type of entertainment. Let books be Class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "faa7f508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26204</th>\n",
       "      <td>“A man craves ultimate truths. Every mortal mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23553</th>\n",
       "      <td>“In our twenties, when there is still so much ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19103</th>\n",
       "      <td>“The fact that, in the United States, there ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18846</th>\n",
       "      <td>“An unrealistic understanding of what living i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21066</th>\n",
       "      <td>“There are some in science who claim time does...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  1\n",
       "26204  “A man craves ultimate truths. Every mortal mi...  0\n",
       "23553  “In our twenties, when there is still so much ...  0\n",
       "19103  “The fact that, in the United States, there ar...  0\n",
       "18846  “An unrealistic understanding of what living i...  0\n",
       "21066  “There are some in science who claim time does...  0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_subset = df1.sample(n = 500)\n",
    "df_books_subset = df_books_subset.drop(df_books_subset.iloc[:, 0:1],axis = 1)\n",
    "df_books_subset = df_books_subset.drop(df_books_subset.iloc[:, 1:5],axis = 1)\n",
    "df_books_subset.columns = [0]\n",
    "df_books_subset[1] = 0\n",
    "df_books_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a437c6",
   "metadata": {},
   "source": [
    "The indicies are not in order, because we randomly selected data from the original dataset. Let us reset the indices so that the dataframe is easier to read. By doing so, we will create a new column that has the original indices--we don't need that, so let us drop that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3d82cade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“A man craves ultimate truths. Every mortal mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“In our twenties, when there is still so much ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“The fact that, in the United States, there ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“An unrealistic understanding of what living i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“There are some in science who claim time does...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  “A man craves ultimate truths. Every mortal mi...  0\n",
       "1  “In our twenties, when there is still so much ...  0\n",
       "2  “The fact that, in the United States, there ar...  0\n",
       "3  “An unrealistic understanding of what living i...  0\n",
       "4  “There are some in science who claim time does...  0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_subset = df_books_subset.reset_index()\n",
    "df_books_subset = df_books_subset.drop(df_books_subset.iloc[:, 0:1],axis = 1)\n",
    "df_books_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd0ccd",
   "metadata": {},
   "source": [
    "It gets more complicated for the movie and video-game datasets, because we have more than one for each type. For the movie datasets, let's take all of the data from the first and third, and the rest from the second. We will wrangle the data exactly as before (dropping unnecessary columns, fixing the indices, selecting random data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65b3072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie dataset 1:  149\n",
      "Movie dataset 2:  293366\n",
      "Movie dataset 3:  99\n"
     ]
    }
   ],
   "source": [
    "print(\"Movie dataset 1: \" , len(df3))\n",
    "print(\"Movie dataset 2: \" , len(df4))\n",
    "print(\"Movie dataset 3: \" , len(df5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0400ec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's showtime!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They're heeeere!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey you guys!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Good morning, Vietnam!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love the smell of napalm in the morning. You...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "2                                     It's showtime!\n",
       "3                                   They're heeeere!\n",
       "4                                      Hey you guys!\n",
       "5                             Good morning, Vietnam!\n",
       "6  I love the smell of napalm in the morning. You..."
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df3.drop(df3.columns[[1,2]], axis = 1)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e17607a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.drop(df4.iloc[:, 0:4],axis = 1)\n",
    "df4 = df4.drop(df4.iloc[:, 1:36],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6ee02469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                4\n",
       "2                                      I hope so.\n",
       "3                                       She okay?\n",
       "4                                       Let's go.\n",
       "5                                             Wow\n",
       "6  Okay -- you're gonna need to learn how to lie."
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e794c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.drop(df5.iloc[:, 0:1],axis = 1)\n",
    "df5 = df5.drop(df5.iloc[:, 1:5],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "dd6843e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm gonna make him an offer he can't refuse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You don't understand! I coulda had class. I co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toto, I've a feeling we're not in Kansas anymore.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Here's looking at you, kid.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Go ahead, make my day.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   1\n",
       "2       I'm gonna make him an offer he can't refuse.\n",
       "3  You don't understand! I coulda had class. I co...\n",
       "4  Toto, I've a feeling we're not in Kansas anymore.\n",
       "5                        Here's looking at you, kid.\n",
       "6                             Go ahead, make my day."
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "36cbe6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns = [0]\n",
    "df4.columns = [0]\n",
    "df5.columns = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bff691b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_subset = df4.sample(n = 500 - len(df3) - len(df5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fcd344",
   "metadata": {},
   "source": [
    "Here, we will use the concat() function from pandas to stack the three datasets on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "082bcb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_subset = pd.concat([df3, df4_subset, df5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3a86f",
   "metadata": {},
   "source": [
    "Let movies be Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8e4f6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Snap out of it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>My mother thanks you. My father thanks you. My...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Nobody puts Baby in a corner.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>I'll get you, my pretty, and your little dog, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>I'm the king of the world!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1\n",
       "495                                    Snap out of it!  1\n",
       "496  My mother thanks you. My father thanks you. My...  1\n",
       "497                      Nobody puts Baby in a corner.  1\n",
       "498  I'll get you, my pretty, and your little dog, ...  1\n",
       "499                         I'm the king of the world!  1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies_subset[1] = 1\n",
    "df_movies_subset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab495d2",
   "metadata": {},
   "source": [
    "Now, we will do the exact same thing to the video-game datasets. Since each one has many lines of dialogue, let us take divide the 500 total lines equally across the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6fc431e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(df2.iloc[:, 0:6],axis = 1)\n",
    "df2 = df2.drop(df2.iloc[:, 1:5],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "67a919ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>They made no demands, gave no hints of their g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>\"In Mundus, conflict and disparity are what br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Verandis and his thirst for knowledge and rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Ahnissi tells you. You are no longer a mewing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Do not pity the lawbreaker.\\r\\n\\tTheir fate is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "161  They made no demands, gave no hints of their g...\n",
       "162  \"In Mundus, conflict and disparity are what br...\n",
       "163  Verandis and his thirst for knowledge and rese...\n",
       "164  Ahnissi tells you. You are no longer a mewing ...\n",
       "165  Do not pity the lawbreaker.\\r\\n\\tTheir fate is..."
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_subset = df2.sample(n = 166)\n",
    "df2_subset = df2_subset.reset_index()\n",
    "df2_subset = df2_subset.drop(df2_subset.iloc[:, 0:1],axis = 1)\n",
    "df2_subset.columns = [0]\n",
    "df2_subset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0334307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df6.drop(df6.iloc[:, 0:3],axis = 1)\n",
    "df6 = df6.drop(df6.iloc[:, 1:7],axis = 1)\n",
    "\n",
    "df6_subset = df6.sample(n = 166)\n",
    "df6_subset = df6_subset.reset_index()\n",
    "df6_subset = df6_subset.drop(df6_subset.iloc[:, 0:1],axis = 1)\n",
    "df6_subset.columns = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6357ff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>The will of the One cannot be denied, Interlop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>I recognize you - you're that new duelist ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Well, that is impressive. I don't get many of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Do you hear that, my master? That is the sound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Ahhh, so you are just another hopeful after al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "161  The will of the One cannot be denied, Interlop...\n",
       "162  I recognize you - you're that new duelist ever...\n",
       "163  Well, that is impressive. I don't get many of ...\n",
       "164  Do you hear that, my master? That is the sound...\n",
       "165  Ahhh, so you are just another hopeful after al..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6_subset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "72188548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df7.drop(df7.iloc[:, 0:8],axis = 1)\n",
    "\n",
    "df7_subset = df7.sample(n = 167)\n",
    "df7_subset = df7_subset.reset_index()\n",
    "df7_subset = df7_subset.drop(df7_subset.iloc[:, 0:1],axis = 1)\n",
    "df7_subset.columns = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a80a937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Now that you have defeated the Ezrohir sorcere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>There's a rumor that the Ezrohir have defeated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>What brought you back here?  Are the Sturmbeor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Thank you again, my friend.  When we've recove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>You have lifted the curse I brought upon mysel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "162  Now that you have defeated the Ezrohir sorcere...\n",
       "163  There's a rumor that the Ezrohir have defeated...\n",
       "164  What brought you back here?  Are the Sturmbeor...\n",
       "165  Thank you again, my friend.  When we've recove...\n",
       "166  You have lifted the curse I brought upon mysel..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7_subset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "925353fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = [0]\n",
    "df6.columns = [0]\n",
    "df7.columns = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6e819d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games_subset = pd.concat([df2_subset, df6_subset, df7_subset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72232d",
   "metadata": {},
   "source": [
    "Let video games be Class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f279a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Why can't Fazeer Shah just open a portal into ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Things are pretty safe here, now.  How are thi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>You've been out there.  You've faced the Ezroh...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Activate the |cFFD1FF7AArmory Portal|u in the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>I have always been led to believe that the Gua...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1\n",
       "494  Why can't Fazeer Shah just open a portal into ...  2\n",
       "495  Things are pretty safe here, now.  How are thi...  2\n",
       "496  You've been out there.  You've faced the Ezroh...  2\n",
       "497  Activate the |cFFD1FF7AArmory Portal|u in the ...  2\n",
       "498  I have always been led to believe that the Gua...  2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_games_subset[1] = 2\n",
    "df_games_subset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc85972",
   "metadata": {},
   "source": [
    "Now, let us take the new datasets for books, movies, and video games, and combine them into one overall dataset for our model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef20b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_subset = pd.concat([df_books_subset, df_movies_subset, df_games_subset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d920f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>Why can't Fazeer Shah just open a portal into ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>Things are pretty safe here, now.  How are thi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>You've been out there.  You've faced the Ezroh...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>Activate the |cFFD1FF7AArmory Portal|u in the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>I have always been led to believe that the Gua...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "1494  Why can't Fazeer Shah just open a portal into ...  2\n",
       "1495  Things are pretty safe here, now.  How are thi...  2\n",
       "1496  You've been out there.  You've faced the Ezroh...  2\n",
       "1497  Activate the |cFFD1FF7AArmory Portal|u in the ...  2\n",
       "1498  I have always been led to believe that the Gua...  2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_subset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c0b1f",
   "metadata": {},
   "source": [
    "Notice that there is an order to our data--books are first, movies are second, and video games are third. This may impact training, so let us scramble all of the data by selecting all of it in random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e899a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_subset = df_combined_subset.sample(n = len(df_combined_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef86eb9",
   "metadata": {},
   "source": [
    "Let us all rename our columns so that the dataframe is easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2f35eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_subset.rename(columns = {0:'line', 1: 'type'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f682a4",
   "metadata": {},
   "source": [
    "Let us ensure that the data in the column 'line' are all Strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5413d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_subset['line'] = df_combined_subset['line'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ecf9d8",
   "metadata": {},
   "source": [
    "Now, let us run a preprocessor() function on all of our quotes. This ensures that our data is \"clean,\" and that every character remaining is useful for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5538917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_subset['line'] = df_combined_subset['line'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ae1f2",
   "metadata": {},
   "source": [
    "Now, let us create our X and y, and use sklearn to split them into testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37a013a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_combined_subset['line'].values\n",
    "y = df_combined_subset['type'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9786cee",
   "metadata": {},
   "source": [
    "From online research, it looks like 3 common models for text classfication are Logistic Regression, SVM, and Naive Bayes (https://github.com/nlptown/nlp-notebooks/blob/master/Traditional%20text%20classification%20with%20Scikit-learn.ipynb). We did not learn about the last model in class. It is based off of Bayes' Theorem, which predicts the probablity of event A, given the fact that event B has occured (https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c). It assumes that features are independent from one another, i.e., feature A does not affect feature B. Our data satisfies this requirement, because we only have one feature to use in our classification.\n",
    "\n",
    "While text classification is a common task for machine learning, I was not able to find previous models that classified entertainment types. This is the main reason why I had to combine so many datasets for my final project. Instead of classifying between entertainment types, other models classified between genres of one type. For example, in 2019, masters' student Amr Shahin at Concordia University classified the genres of movies by using their scripts (https://spectrum.library.concordia.ca/id/eprint/985410/1/Shahin_MSc_S2019.pdf.pdf). Using HAN (\"hierarchical attention network\") and CNN models, he was able to achieve accuracy scores ranging from 88-95%. Since I could not find previous experiments of my task, I determined that I would call my final project a success if my accuracy score reached 70% (lower than Shahin's accuracy, yet still better than a coin flip).\n",
    "\n",
    "For all models, my dataset will be transformed from text to TF-IDF features using the TfidVectorizer() function from sklearn. TF-IDF stands for \"term frequency\" and \"inverse document frequency.\" The former term is self-explanatory: the feature keeps count of how many times a word appears in an instance of the data. The latter term measures the \"relative rarity of a term\" throughout all instances of the data. Thus, TF-IDF \"uses the frequency of words to determine how relevant those words are to a given document\" (https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/). Alternatives to TF-IDF is the \"bag-of-words\" approach, which simply counts the frequencies of each word, and the BERT (Bidirectional Encoder Representations from Transformers) approach, which builds upon TF-IDF to include the semantic meanings of words. The former was taught in class, and the latter was suggested by Will Wang. I have chosen to use TF-IDF because it was used in a text-classification problem in the textbook.\n",
    "\n",
    "Let us first test logistic regression on our subset of data.\n",
    "\n",
    "In the textbook, they performed a grid search before using a model. Let us do the same. By performing grid search, we will find the optimal set of hyperparameters for our model. For our grid search, we will also tokenize our lines (so that the model can analyze word by word instead of sentence by sentence), or use the Porter stemming algorithm to reduce words to their \"root form\" (ex. running -> run). We will also include the option of removing stopwords in English, which are common and unhelpful words, such as \"and.\" \n",
    "\n",
    "I did want to include more parameters, but was limited by the amount of computation power. I chose those we talked about in class, as well as the ones included in the textbook. The original code and idea came from the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94623356",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.2s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   1.9s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   2.4s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   2.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.7s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.6s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=   0.5s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.01, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=0.1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=1, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=10, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.3s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=ovr, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.2s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.3s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.3s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.3s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.3s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.3s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l2, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.1s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=newton-cg, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n",
      "[CV] END clf__C=100, clf__multi_class=multinomial, clf__penalty=l1, clf__solver=sag, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "600 fits failed out of a total of 1200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1519, in fit\n",
      "    multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))\n",
      "  File \"/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 483, in _check_multi_class\n",
      "    raise ValueError(\"Solver %s does not support a multinomial backend.\" % solver)\n",
      "ValueError: Solver liblinear does not support a multinomial backend.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.55196628 0.55290499 0.61392117 0.62154021 0.55196628 0.55290499\n",
      "        nan        nan 0.34032354 0.34032354        nan        nan\n",
      " 0.61776259 0.61966735        nan        nan 0.61871953 0.61966735\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65013443 0.65205286 0.64919116 0.6529779  0.65013443 0.65205286\n",
      "        nan        nan 0.51380725 0.51095466        nan        nan\n",
      " 0.65871953 0.65298701        nan        nan 0.65871953 0.65298701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69398952 0.68826156 0.69113238 0.69016632 0.69398952 0.68826156\n",
      "        nan        nan 0.61677831 0.62251082        nan        nan\n",
      " 0.69209387 0.70065163        nan        nan 0.69209387 0.70065163\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70734108 0.71593074 0.70734108 0.71593074 0.70734108 0.71593074\n",
      "        nan        nan 0.67874687 0.67682843        nan        nan\n",
      " 0.70639326 0.71497835        nan        nan 0.70639326 0.71402597\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70257006 0.71305992 0.70161768 0.71401686 0.70733652 0.71306448\n",
      "        nan        nan 0.67492367 0.67207564        nan        nan\n",
      " 0.70352244 0.70161312        nan        nan 0.70161768 0.70733197\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60058328 0.6624926  0.65394851 0.66825245 0.64151743 0.5643609\n",
      "        nan        nan 0.34032354 0.47570745        nan        nan\n",
      " 0.60914103 0.65298701        nan        nan 0.67394395 0.60631123\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65773525 0.68638414 0.66536797 0.68353156 0.66440191 0.57865117\n",
      "        nan        nan 0.52238323 0.63296423        nan        nan\n",
      " 0.67488722 0.69021189        nan        nan 0.68251082 0.62633402\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69110504 0.6949738  0.69110959 0.69592618 0.66440647 0.58055138\n",
      "        nan        nan 0.66249715 0.68542265        nan        nan\n",
      " 0.68061973 0.69305992        nan        nan 0.67870586 0.62538164\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67489177 0.6940123  0.67489177 0.69496468 0.66250171 0.58246981\n",
      "        nan        nan 0.64537708 0.6758715         nan        nan\n",
      " 0.66916838 0.6854272         nan        nan 0.67774436 0.62538164\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65297334 0.68734564 0.65392572 0.6863887  0.66154933 0.58055594\n",
      "        nan        nan 0.61392572 0.6587332         nan        nan\n",
      " 0.63963545 0.6749419         nan        nan 0.67679198 0.62729095\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf', LogisticRegression())]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{'clf__C': [0.01, 0.1, 1, 10, 100],\n",
       "                          'clf__multi_class': ['ovr', 'multinomial'],\n",
       "                          'clf__penalty': ['l2', 'l1'],\n",
       "                          'clf__solver': ['newton-cg', 'liblinear', 'sag'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokeniz...\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None)\n",
    "\n",
    "param_grid = [\n",
    "{\n",
    "'vect__ngram_range': [(1, 1)],\n",
    "'vect__stop_words': [None],\n",
    "'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "'clf__penalty': ['l2', 'l1'],\n",
    "'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "'clf__solver': ['liblinear', 'newton-cg', 'sag'],\n",
    "'clf__multi_class': ['ovr', 'multinomial']\n",
    "},\n",
    "{\n",
    "'vect__ngram_range': [(1, 1)],\n",
    "'vect__stop_words': [stop, None],\n",
    "'vect__tokenizer': [tokenizer],\n",
    "'vect__use_idf':[False],\n",
    "'vect__norm':[None],\n",
    "'clf__penalty': ['l2', 'l1'],\n",
    "'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "'clf__solver': ['liblinear', 'newton-cg', 'sag'],\n",
    "'clf__multi_class': ['ovr', 'multinomial']\n",
    "},\n",
    "]\n",
    "\n",
    "logistic_regression_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression())])\n",
    "\n",
    "grid_search = GridSearchCV(logistic_regression_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0063c18",
   "metadata": {},
   "source": [
    "Here is the best parameter set that our Grid Search found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5b86752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10, 'clf__multi_class': 'ovr', 'clf__penalty': 'l2', 'clf__solver': 'newton-cg', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x7fbc905e8c10>}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e879bfa",
   "metadata": {},
   "source": [
    "Let us look at the confusion matrix, the classification report, and the accuracy score.\n",
    "\n",
    "We can see that our model has achieved an accuracy score of 72.8%. The scores for precision, recall, and f1-score all hover around 70%. Thus, the model does not make more mistakes with one type than the other (such as false positives vs. false negatives).\n",
    "\n",
    "The performance of our model can be seen in the confusion matrix, in which the light green and yellow colors indicate the highest frequencies of predictions and the purple color indicates the lowest frequencies. The clear diagonal line shows that our model is correct for most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d99b0aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72       143\n",
      "           1       0.69      0.72      0.70       150\n",
      "           2       0.77      0.75      0.76       157\n",
      "\n",
      "    accuracy                           0.73       450\n",
      "   macro avg       0.73      0.73      0.73       450\n",
      "weighted avg       0.73      0.73      0.73       450\n",
      "\n",
      "Accuracy Score:  0.7288888888888889\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "910d5448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf18ef0",
   "metadata": {},
   "source": [
    "Let's train the model on the entire dataset now. If it achieves a higher accuracy score, then we will implement and train the remaining models (random forest, support vector machine, and naive bayesian) on the entire dataset. If not, then we will train the models on the subsets that were defined before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "21660b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29994</th>\n",
       "      <td>“To be, or not to be: that is the question:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>“I always think of each night as a song. Or ea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>“It's not what I'd want for at my funeral. Whe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>“Người quyết tâm đào tạo cho mình một đời sống...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>“I believe that to be an efficient writer, one...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  1\n",
       "29994        “To be, or not to be: that is the question:  0\n",
       "29995  “I always think of each night as a song. Or ea...  0\n",
       "29996  “It's not what I'd want for at my funeral. Whe...  0\n",
       "29997  “Người quyết tâm đào tạo cho mình một đời sống...  0\n",
       "29998  “I believe that to be an efficient writer, one...  0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with the books\n",
    "df_books = df1\n",
    "df_books = df_books.drop(df_books.iloc[:, 0:1],axis = 1)\n",
    "df_books = df_books.drop(df_books.iloc[:, 1:5],axis = 1)\n",
    "df_books.columns = [0]\n",
    "df_books[1] = 0\n",
    "df_books = df_books.reset_index()\n",
    "df_books = df_books.drop(df_books.iloc[:, 0:1],axis = 1)\n",
    "df_books.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "36657769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>293609</th>\n",
       "      <td>Snap out of it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293610</th>\n",
       "      <td>My mother thanks you. My father thanks you. My...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293611</th>\n",
       "      <td>Nobody puts Baby in a corner.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293612</th>\n",
       "      <td>I'll get you, my pretty, and your little dog, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293613</th>\n",
       "      <td>I'm the king of the world!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0  1\n",
       "293609                                    Snap out of it!  1\n",
       "293610  My mother thanks you. My father thanks you. My...  1\n",
       "293611                      Nobody puts Baby in a corner.  1\n",
       "293612  I'll get you, my pretty, and your little dog, ...  1\n",
       "293613                         I'm the king of the world!  1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then, move on to the movies.\n",
    "df_movies = pd.concat([df3, df4, df5], ignore_index=True)\n",
    "df_movies[1] = 1\n",
    "df_movies.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "406f5b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35659</th>\n",
       "      <td>- Locate the \"|cFFD1FF7ANether-Rift|u\" in the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35660</th>\n",
       "      <td>- Close the \"|cFFD1FF7ANether-Rift|u\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35661</th>\n",
       "      <td>Somebody help me!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35662</th>\n",
       "      <td>Thanks for your help.  Here ... something for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35663</th>\n",
       "      <td>Defeat the monsters attacking the traveler's w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  1\n",
       "35659  - Locate the \"|cFFD1FF7ANether-Rift|u\" in the ...  2\n",
       "35660              - Close the \"|cFFD1FF7ANether-Rift|u\"  2\n",
       "35661                                  Somebody help me!  2\n",
       "35662  Thanks for your help.  Here ... something for ...  2\n",
       "35663  Defeat the monsters attacking the traveler's w...  2"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, create the video-game dataset.\n",
    "df_games = pd.concat([df2, df6, df7], ignore_index=True)\n",
    "df_games[1] = 2\n",
    "df_games.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "670a2ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>359272</th>\n",
       "      <td>- Locate the \"|cFFD1FF7ANether-Rift|u\" in the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359273</th>\n",
       "      <td>- Close the \"|cFFD1FF7ANether-Rift|u\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359274</th>\n",
       "      <td>Somebody help me!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359275</th>\n",
       "      <td>Thanks for your help.  Here ... something for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359276</th>\n",
       "      <td>Defeat the monsters attacking the traveler's w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0  1\n",
       "359272  - Locate the \"|cFFD1FF7ANether-Rift|u\" in the ...  2\n",
       "359273              - Close the \"|cFFD1FF7ANether-Rift|u\"  2\n",
       "359274                                  Somebody help me!  2\n",
       "359275  Thanks for your help.  Here ... something for ...  2\n",
       "359276  Defeat the monsters attacking the traveler's w...  2"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the combined dataset.\n",
    "df_all = pd.concat([df_books, df_movies, df_games], ignore_index=True)\n",
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "94866d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again, let us scramble the data so that there is no order to the classes, rename the columns, convert \n",
    "# entry to a string, and apply the preprocessor() function.\n",
    "df_all = df_all.sample(n = len(df_all))\n",
    "df_all.rename(columns = {0:'line', 1: 'type'}, inplace = True)\n",
    "df_all['line'] = df_all['line'].astype(str)\n",
    "df_all['line'] = df_all['line'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "16aa8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_all['line'].values\n",
    "y = df_all['type'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dff68",
   "metadata": {},
   "source": [
    "Now, let us train a logistic regression model in the same way as before. It is going to take a much longer time,\n",
    "due to the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0eaa2f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   5.1s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   6.0s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   5.5s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   5.1s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   5.0s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  53.3s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  53.0s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  54.7s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  53.3s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  53.3s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   8.2s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   8.0s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   7.8s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   8.0s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   8.2s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  56.2s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  57.1s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  57.2s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  57.8s\n",
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  59.1s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  13.1s\n",
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  15.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  12.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  31.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  32.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  30.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  33.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=1, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  33.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  25.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  22.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  20.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  21.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  23.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  52.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  50.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  49.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopetsai/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END clf__C=10, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=  54.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(solver='liblinear'))]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{'clf__C': [1, 10], 'clf__penalty': ['l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>,\n",
       "                                              <function tokenizer_porter at 0x7fbc905e8c10>]},\n",
       "                         {...\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "{\n",
    "'vect__ngram_range': [(1, 1)],\n",
    "'vect__stop_words': [None],\n",
    "'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "'clf__penalty': ['l2'],\n",
    "'clf__C': [1, 10]\n",
    "},\n",
    "{\n",
    "'vect__ngram_range': [(1, 1)],\n",
    "'vect__stop_words': [stop, None],\n",
    "'vect__tokenizer': [tokenizer],\n",
    "'vect__use_idf':[False],\n",
    "'vect__norm':[None],\n",
    "'clf__penalty': ['l2'],\n",
    "'clf__C': [1, 10]\n",
    "},\n",
    "]\n",
    "\n",
    "logistic_regression_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "grid_search = GridSearchCV(logistic_regression_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b72433-d378-4091-ad44-184141d47e1f",
   "metadata": {},
   "source": [
    "I have deleted the three additional hyperparameters (C, solver, multi_class) which weren't in the textbook's version of the code. As we have much more data now, the model will take a much longer time to train. Due to pressing deadlines, I chose to delete these hyperparameters in order to make my models more efficient.\n",
    "\n",
    "Looking at the accuracy score of 92.3%, we can see that, despite this, our model still performed much better once trained on the entire dataset. However, the model is no longer equally good at text classification. By our classification report, we can see that the recall score for books (class 0) is only 64%. This means that our model has many \"false negatives\"--it predicts that a line from a book is NOT from a book. Interestingly enough, the recall score for video games (class 2) is also low--67%. The high recall score for movies (class 1) indicates that our model is great at classifying movies. This dataset has the most data, which may be a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32dcef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7fbc905e8ca0>}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3dfcc86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.64      0.72      8985\n",
      "           1       0.93      0.98      0.96     87992\n",
      "           2       0.91      0.67      0.77     10807\n",
      "\n",
      "    accuracy                           0.92    107784\n",
      "   macro avg       0.89      0.76      0.82    107784\n",
      "weighted avg       0.92      0.92      0.92    107784\n",
      "\n",
      "Accuracy Score:  0.9232446374229941\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "278dc252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2caed3e",
   "metadata": {},
   "source": [
    "We achieved a higher accuracy score when we trained our model on all the data. So, let's use this dataset to train our remaining models. First, we have random forest, which is a combination of many decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2b133a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time= 9.4min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time= 9.7min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time= 9.5min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time= 9.6min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time= 9.4min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time= 8.9min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time= 8.7min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time= 8.8min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time= 9.1min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time= 8.9min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=13.3min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=13.6min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=13.9min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=14.0min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=14.1min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=11.7min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=11.3min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=11.0min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=11.1min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=11.1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf', RandomForestClassifier())]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>,\n",
       "                                              <function tokenizer_porter at 0x7fbc905e8c10>]},\n",
       "                         {'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vec...': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "{\n",
    "'vect__ngram_range': [(1, 1)],\n",
    "'vect__stop_words': [None],\n",
    "'vect__tokenizer': [tokenizer, tokenizer_porter]},\n",
    "{\n",
    "'vect__ngram_range': [(1, 1)],\n",
    "'vect__stop_words': [stop, None],\n",
    "'vect__tokenizer': [tokenizer],\n",
    "'vect__use_idf':[False],\n",
    "'vect__norm':[None]}\n",
    "]\n",
    "\n",
    "random_forest_tfidf = Pipeline([('vect', tfidf), ('clf', RandomForestClassifier())])\n",
    "\n",
    "grid_search = GridSearchCV(random_forest_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e6f3a70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'vect__ngram_range': (1, 1), 'vect__norm': None, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenizer at 0x7fbc905e8ca0>, 'vect__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa331c07-555c-4ab6-bc8d-f3d3c149e39b",
   "metadata": {},
   "source": [
    "Our accuracy score is lower than the accuracy score with logistic regression on the entire dataset--it is 89.3%, roughly 2 percentage points lower. The random forest model is even worse at \"equal performances\" throughout the three classes. For example, the recall score for books is only 40%. Thus, I prefer logistic regression over random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1ac9be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.40      0.54      8985\n",
      "           1       0.90      0.99      0.94     87992\n",
      "           2       0.89      0.55      0.68     10807\n",
      "\n",
      "    accuracy                           0.89    107784\n",
      "   macro avg       0.87      0.64      0.72    107784\n",
      "weighted avg       0.89      0.89      0.88    107784\n",
      "\n",
      "Accuracy Score:  0.8931659615527351\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c3240161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a0897",
   "metadata": {},
   "source": [
    "Now, let us train an SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0863a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=105.5min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=105.3min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=104.8min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=107.6min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=120.9min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=101.7min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=102.0min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=103.6min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=97.3min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=86.0min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=66.2min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=65.7min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=59.7min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=58.0min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=59.4min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=60.8min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=63.9min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=56.3min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=61.8min\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=62.3min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf', SVC())]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>,\n",
       "                                              <function tokenizer_porter at 0x7fbc905e8c10>]},\n",
       "                         {'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_tfidf = Pipeline([('vect', tfidf), ('clf', svm.SVC())])\n",
    "\n",
    "grid_search = GridSearchCV(svm_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ef7eeb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x7fbc905e8c10>}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00558d19-9289-49d0-bf8d-1f1b7a19fe29",
   "metadata": {},
   "source": [
    "The SVM model performs only slightly better than the logistic regression model. It has an accuracy score of 92.5%, which is only 0.3% higher than the logistic regression model, and has recall scores that are similar to the other model (65% for books). Thus, if I were to recommend a model for this task, I would choose either the SVM model or the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fea04a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      8985\n",
      "           1       0.93      0.99      0.96     87992\n",
      "           2       0.97      0.63      0.77     10807\n",
      "\n",
      "    accuracy                           0.93    107784\n",
      "   macro avg       0.92      0.76      0.82    107784\n",
      "weighted avg       0.93      0.93      0.92    107784\n",
      "\n",
      "Accuracy Score:  0.9259166481110369\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "36872dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb6c5c",
   "metadata": {},
   "source": [
    "Lastly, we train a Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "38e41511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   2.0s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   1.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   1.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   1.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   2.1s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  51.1s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  50.0s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  49.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  51.3s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  51.4s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.6s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.6s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.6s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.6s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.6s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf', MultinomialNB())]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>,\n",
       "                                              <function tokenizer_porter at 0x7fbc905e8c10>]},\n",
       "                         {'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vect__stop_w...': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_tfidf = Pipeline([('vect', tfidf), ('clf', MultinomialNB())])\n",
    "\n",
    "grid_search = GridSearchCV(naive_bayes_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9394dd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'vect__ngram_range': (1, 1), 'vect__norm': None, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenizer at 0x7fbc905e8ca0>, 'vect__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45defe9d-2cdd-4c4b-bd55-72d50404076e",
   "metadata": {},
   "source": [
    "The Naive Bayes model is slightly worse than the SVM and logistic regression models; it has an accuracy score of 90%. At the same time, its classification report looks very similar to those of the mentioned models. For example, the recall score for books is 62%. Thus, I would not prefer this model over the SVM or logistic regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d3605752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.62      0.70      8985\n",
      "           1       0.92      0.97      0.95     87992\n",
      "           2       0.78      0.59      0.67     10807\n",
      "\n",
      "    accuracy                           0.90    107784\n",
      "   macro avg       0.83      0.73      0.77    107784\n",
      "weighted avg       0.90      0.90      0.90    107784\n",
      "\n",
      "Accuracy Score:  0.9035200029689008\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4bab1e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86e431",
   "metadata": {},
   "source": [
    "Notice that Class 1 (dialogue from movies) has a significantly higher quantity of data. Let's see how much the classes are imbalanced, and edit the dataframe to achieve balanced classes. We'll use the naive bayes model, simply because it was one of the faster models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "925fc392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    293614\n",
       "2     35664\n",
       "0     29999\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "34358786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's divide the Movies class by a factor of 10, so that the classes are relatively balanced.\n",
    "df_movies = df_movies.sample(n = int(len(df_movies)/10))\n",
    "df_movies = df_movies.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8e010b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The same thing.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On his death bed I promised Marcus I would com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You can't remember that -- !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the chance you've been waiting for boy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0                                    The same thing.  1\n",
       "1  On his death bed I promised Marcus I would com...  1\n",
       "2                       You can't remember that -- !  1\n",
       "3                                           Stop it.  1\n",
       "4  This is the chance you've been waiting for boy...  1"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies = df_movies.drop(df_movies.iloc[:, 0:1],axis = 1)\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "10822251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37573</th>\n",
       "      <td>i bet you like sittin between two men in a dua...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41069</th>\n",
       "      <td>she s not that either</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80426</th>\n",
       "      <td>gerlon losing to a rookie is this a sign that ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50366</th>\n",
       "      <td>it s going to be a long night it could take ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40942</th>\n",
       "      <td>the problem is i don t happen to think my son ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    line  type\n",
       "37573  i bet you like sittin between two men in a dua...     1\n",
       "41069                             she s not that either      1\n",
       "80426  gerlon losing to a rookie is this a sign that ...     2\n",
       "50366  it s going to be a long night it could take ho...     1\n",
       "40942  the problem is i don t happen to think my son ...     1"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.concat([df_books, df_movies, df_games], ignore_index=True)\n",
    "df_all = df_all.sample(n = len(df_all))\n",
    "df_all.rename(columns = {0:'line', 1: 'type'}, inplace = True)\n",
    "df_all['line'] = df_all['line'].astype(str)\n",
    "df_all['line'] = df_all['line'].apply(preprocessor)\n",
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b53f5c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_all['line'].values\n",
    "y = df_all['type'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "20c70458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    35664\n",
       "0    29999\n",
       "1    29361\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "11121e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>; total time=   0.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  28.3s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  28.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  28.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  28.6s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fbc905e8c10>; total time=  28.5s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.8s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   1.0s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.9s\n",
      "[CV] END vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fbc905e8ca0>, vect__use_idf=False; total time=   0.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf', MultinomialNB())]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>,\n",
       "                                              <function tokenizer_porter at 0x7fbc905e8c10>]},\n",
       "                         {'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vect__stop_w...': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fbc905e8ca0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_tfidf = Pipeline([('vect', tfidf), ('clf', MultinomialNB())])\n",
    "\n",
    "grid_search = GridSearchCV(naive_bayes_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cc8ff0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x7fbc905e8c10>}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bad244-3e81-4b59-bd39-2b604eb0876c",
   "metadata": {},
   "source": [
    "The accuracy score is much worse than the other models--it is 78.1%. However, it succeeds in performing \"equally across classes.\" The classification report shows that it is does not falter with a specific class or instance (such as predicting false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1e265582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82      9071\n",
      "           1       0.84      0.60      0.70      8688\n",
      "           2       0.72      0.91      0.80     10749\n",
      "\n",
      "    accuracy                           0.78     28508\n",
      "   macro avg       0.80      0.77      0.77     28508\n",
      "weighted avg       0.79      0.78      0.78     28508\n",
      "\n",
      "Accuracy Score:  0.7819910200645432\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0bc46459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb97fe-3db3-436e-8522-93acae5f1151",
   "metadata": {},
   "source": [
    "Thus, the best model was the support vector machine, which had an accuracy score of 92.5%. The logistic regression model was a close second, with an accuracy score of 92.3%. As expected, the models performed better when they were trained on the entire dataset. However, what surprised me was the fact that the Naive Bayes model performed worse on a balanced subset (accuracy score: 78.1%) than on the entire dataset (accuracy score: 90.3%). I did not expect that removing one-tenth of the data from the movie dataset would impact the accuracy score so much.\n",
    "\n",
    "Moreover, the classification report for the Naive Bayes model on the balanced subset shows that the model performs equally well on all three classes. This would be ideal in \"life-or-death\" situations, such as in the world of healthcare, but is not needed when it comes to predicting the entertainment type of a quote. Thus, for this task, I would reccommend training the SVM model on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332afd3d-7cb7-4544-826c-6a7415e43d08",
   "metadata": {},
   "source": [
    "In the future, I hope to test all of the hyperparameters in gridsearch. (I did attempt to use Amherst's new cluster for my final project, but the kernel ran out of memory.) Thus, I would be able to exhaust all possibilities, and can truly know that the accuracy score is the highest it can be.\n",
    "\n",
    "In the future, I also hope to train and test my models on more data. I noticed that my video-game data were all based in fantastical video games, while my book and film data came from a variety of genres. I wonder if this imbalance affected my models. In order to answer this question, I hope to find more video game dialogue that spans all genres. I also hope to find more book dialogue, to see if the recall and precision scores would improve once I had a balanced dataset.\n",
    "\n",
    "It would also be interesting to include video games, books, and movies of the same subject matter (such as Star Wars or Harry Potter) to see if the model can distinguish between them.  \n",
    "\n",
    "Finally, if I had additional time, I would like to test the HAN model, which Shahin used in his thesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06877b46-150b-443c-b86b-28cee41bcab1",
   "metadata": {},
   "source": [
    "**Ultimately, using machine learning, we can conclude that there is a discernable difference between books, movies, and video games. The social impact of my final project supports the age-old parental preference of books over movies and video games, and provides a more objective (as a model is not emotionally involved in the lives of humans)reason to this preference (even though there is bias in my data, as stated above).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
